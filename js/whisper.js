/**
 * Whisper AI Integration using Transformers.js
 * ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏•‡∏≥‡πÇ‡∏û‡∏á
 */

import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';

// Configure
env.allowLocalModels = false;
env.allowRemoteModels = true;

// Global variables
let transcriber = null;
let isModelLoaded = false;

// ===========================================
// Initialize Whisper Model
// ===========================================
async function initializeWhisper() {
    try {
        console.log('üì• Loading Whisper model...');
        updateModelStatus('loading', '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î AI Model...');
        
        // Load Whisper tiny model (smallest, fastest)
        transcriber = await pipeline(
            'automatic-speech-recognition',
            'Xenova/whisper-tiny',
            {
                quantized: true,  // ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á
            }
        );
        
        isModelLoaded = true;
        console.log('‚úÖ Whisper model loaded!');
        updateModelStatus('ready', 'AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô');
        showToast('‚úÖ AI Model ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!', 'success');
        
    } catch (error) {
        console.error('‚ùå Failed to load model:', error);
        updateModelStatus('error', '‡πÇ‡∏´‡∏•‡∏î Model ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à');
        showToast('‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î AI Model ‡πÑ‡∏î‡πâ: ' + error.message, 'error');
    }
}

// Update model status UI
function updateModelStatus(status, text) {
    const statusEl = document.getElementById('modelStatus');
    if (!statusEl) return;
    
    const icons = {
        loading: '<i class="fas fa-circle-notch fa-spin"></i>',
        ready: '<i class="fas fa-check-circle" style="color: var(--success);"></i>',
        error: '<i class="fas fa-exclamation-circle" style="color: var(--danger);"></i>',
        processing: '<i class="fas fa-cog fa-spin"></i>'
    };
    
    statusEl.innerHTML = `${icons[status]} <span>${text}</span>`;
}

// ===========================================
// Transcribe Audio File
// ===========================================
async function transcribeAudioFile(audioFile, onProgress) {
    try {
        // Check if model is loaded
        if (!isModelLoaded || !transcriber) {
            showToast('‚ö†Ô∏è AI Model ‡∏¢‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà...', 'warning');
            await initializeWhisper();
        }
        
        console.log('üé§ Starting transcription...');
        updateModelStatus('processing', '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á...');
        
        // Read audio file
        const audioData = await readAudioFile(audioFile);
        
        // Transcribe
        const result = await transcriber(audioData, {
            language: 'thai',  // ‡∏´‡∏£‡∏∑‡∏≠ 'english'
            task: 'transcribe',
            chunk_length_s: 30,
            stride_length_s: 5,
            return_timestamps: true,
            callback_function: (data) => {
                // Progress callback
                if (onProgress && data.status) {
                    onProgress(data);
                }
            }
        });
        
        console.log('‚úÖ Transcription complete:', result);
        updateModelStatus('ready', 'AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô');
        
        return result;
        
    } catch (error) {
        console.error('‚ùå Transcription error:', error);
        updateModelStatus('error', '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î');
        throw error;
    }
}

// ===========================================
// Read Audio File
// ===========================================
async function readAudioFile(file) {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        
        reader.onload = async (e) => {
            try {
                const arrayBuffer = e.target.result;
                
                // Decode audio
                const audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 16000  // Whisper requires 16kHz
                });
                
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Get audio data (mono channel)
                const audioData = audioBuffer.getChannelData(0);
                
                resolve(audioData);
                
            } catch (error) {
                reject(error);
            }
        };
        
        reader.onerror = () => reject(reader.error);
        reader.readAsArrayBuffer(file);
    });
}

// ===========================================
// Initialize on load
// ===========================================
window.addEventListener('load', () => {
    initializeWhisper();
});

// Export functions
window.transcribeAudioFile = transcribeAudioFile;
window.isWhisperReady = () => isModelLoaded;

console.log('‚úÖ Whisper module loaded');
